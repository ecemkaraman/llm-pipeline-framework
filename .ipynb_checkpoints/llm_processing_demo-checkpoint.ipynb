{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83d\ude80 Token Processing in LLMs: Step-by-Step Demonstration\n", "### \ud83d\udccc Understand how text is transformed inside an LLM, from Token IDs to Self-Attention\n", "This notebook will guide you through:\n", "- \u2705 Tokenization (Converting text into Token IDs)\n", "- \u2705 Embedding Layer (Mapping tokens to dense vectors)\n", "- \u2705 Positional Encoding (Adding order information)\n", "- \u2705 Self-Attention (Computing relationships between words)\n", "- \u2705 Multi-Head Attention & Feedforward Network\n"]}, {"cell_type": "code", "metadata": {}, "source": ["# Install required libraries\n", "!pip install transformers numpy torch matplotlib"]}, {"cell_type": "code", "metadata": {}, "source": ["import numpy as np\n", "import torch\n", "import torch.nn as nn\n", "import matplotlib.pyplot as plt\n", "from transformers import AutoTokenizer, AutoModel\n", "import math"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udd39 Define Sample Input Text & Tokenization\n", "We'll tokenize the sentence into Token IDs."]}, {"cell_type": "code", "metadata": {}, "source": ["# Load a tokenizer (GPT-2 as an example)\n", "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n", "\n", "# Define sample text\n", "text = \"The cat sat on the mat\"\n", "tokens = tokenizer.tokenize(text)\n", "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n", "\n", "print(\"\ud83d\udd39 Tokens:\", tokens)\n", "print(\"\ud83d\udd39 Token IDs:\", token_ids)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udd39 Embedding Layer: Converting Token IDs into Dense Vectors\n", "Each token ID is mapped to a vector in a high-dimensional space."]}, {"cell_type": "code", "metadata": {}, "source": ["# Define an embedding layer (Assume embedding size is 8 for demonstration)\n", "embedding_layer = nn.Embedding(num_embeddings=tokenizer.vocab_size, embedding_dim=8)\n", "embedded_tokens = embedding_layer(torch.tensor(token_ids))\n", "\n", "print(\"\ud83d\udd39 Embedded Token Shape:\", embedded_tokens.shape)\n", "print(\"\ud83d\udd39 First Token Embedding:\", embedded_tokens[0].detach().numpy())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udd39 Positional Encoding: Adding Order Information to Tokens\n", "Since Transformers process words in parallel, positional encoding ensures word order is retained."]}, {"cell_type": "code", "metadata": {}, "source": ["def positional_encoding(seq_length, d_model):\n", "    pe = np.zeros((seq_length, d_model))\n", "    for pos in range(seq_length):\n", "        for i in range(0, d_model, 2):\n", "            pe[pos, i] = math.sin(pos / (10000 ** (i / d_model)))\n", "            pe[pos, i + 1] = math.cos(pos / (10000 ** (i / d_model)))\n", "    return torch.tensor(pe, dtype=torch.float32)\n", "\n", "# Apply positional encoding\n", "pos_encoding = positional_encoding(len(token_ids), 8)\n", "print(\"\ud83d\udd39 Positional Encoding Shape:\", pos_encoding.shape)\n", "print(\"\ud83d\udd39 First Positional Encoding Vector:\", pos_encoding[0].numpy())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udd39 Self-Attention Mechanism: Computing Word Relationships\n", "We compute Query, Key, and Value matrices to find word importance."]}, {"cell_type": "code", "metadata": {}, "source": ["d_model = 8  # Embedding dimension\n", "\n", "# Define Q, K, V transformation matrices\n", "W_Q = nn.Linear(d_model, d_model)\n", "W_K = nn.Linear(d_model, d_model)\n", "W_V = nn.Linear(d_model, d_model)\n", "\n", "# Compute Q, K, V\n", "Q = W_Q(embedded_tokens)\n", "K = W_K(embedded_tokens)\n", "V = W_V(embedded_tokens)\n", "\n", "# Compute Attention Scores (Scaled Dot Product)\n", "scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_model)\n", "attention_weights = torch.nn.functional.softmax(scores, dim=-1)\n", "\n", "print(\"\ud83d\udd39 Attention Scores Shape:\", scores.shape)\n", "print(\"\ud83d\udd39 Attention Weights:\", attention_weights.detach().numpy())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udd39 Apply Attention to Value V\n", "The weighted sum of values determines the final representation."]}, {"cell_type": "code", "metadata": {}, "source": ["output = torch.matmul(attention_weights, V)\n", "print(\"\ud83d\udd39 Attention Output Shape:\", output.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udd39 Multi-Head Attention Simulation\n", "Instead of one attention mechanism, we use multiple heads."]}, {"cell_type": "code", "metadata": {}, "source": ["num_heads = 2  # Simulating two attention heads\n", "multi_head_output = torch.cat([output, output], dim=-1)\n", "print(\"\ud83d\udd39 Multi-Head Output Shape:\", multi_head_output.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udd39 Feedforward Layer: Refining Representations\n", "Final transformation to enrich context before moving to the next Transformer block."]}, {"cell_type": "code", "metadata": {}, "source": ["feedforward = nn.Sequential(\n", "    nn.Linear(d_model * 2, 32),\n", "    nn.ReLU(),\n", "    nn.Linear(32, d_model * 2)\n", ")\n", "\n", "final_representation = feedforward(multi_head_output)\n", "print(\"\ud83d\udd39 Final Representation Shape:\", final_representation.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83c\udfaf Summary: How Text is Processed in an LLM\n", "- \u2705 Tokenization \u2192 Converts text into token IDs\n", "- \u2705 Embedding Layer \u2192 Maps tokens into dense numerical vectors\n", "- \u2705 Positional Encoding \u2192 Adds order information using sine & cosine waves\n", "- \u2705 Self-Attention \u2192 Determines which words are important in context\n", "- \u2705 Multi-Head Attention \u2192 Enhances word relationships from different perspectives\n", "- \u2705 Feedforward Network \u2192 Further refines contextual embeddings\n", "\n", "\ud83d\ude80 **This is the foundation of how Transformers process text!**"]}], "metadata": {}, "nbformat": 4, "nbformat_minor": 4}