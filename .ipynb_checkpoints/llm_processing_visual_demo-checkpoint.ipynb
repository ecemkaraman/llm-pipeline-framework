{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83d\ude80 Visualizing Token Embeddings & Self-Attention in LLMs\n", "### \ud83d\udccc This notebook will demonstrate how a Large Language Model (LLM) processes text **visually**, covering:\n", "- \u2705 Tokenization (Converting text into Token IDs)\n", "- \u2705 Embedding Layer (Mapping tokens to dense vectors)\n", "- \u2705 Positional Encoding (Adding order information)\n", "- \u2705 Self-Attention (Visualizing Attention Scores)\n", "- \u2705 Multi-Head Attention & Feedforward Networks\n"]}, {"cell_type": "code", "metadata": {}, "source": ["# Install required libraries\n", "!pip install transformers numpy torch matplotlib seaborn"]}, {"cell_type": "code", "metadata": {}, "source": ["import numpy as np\n", "import torch\n", "import torch.nn as nn\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from transformers import AutoTokenizer\n", "import math"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udd39 Tokenization: Converting Text into Token IDs\n", "We'll tokenize the sentence into Token IDs using a pre-trained tokenizer."]}, {"cell_type": "code", "metadata": {}, "source": ["# Load a tokenizer (GPT-2 as an example)\n", "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n", "\n", "# Define sample text\n", "text = \"The cat sat on the mat\"\n", "tokens = tokenizer.tokenize(text)\n", "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n", "\n", "print(\"\ud83d\udd39 Tokens:\", tokens)\n", "print(\"\ud83d\udd39 Token IDs:\", token_ids)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udd39 Embedding Layer: Mapping Token IDs to Dense Vectors\n", "Each token ID is mapped to a vector in a high-dimensional space.\n", "### \ud83d\udccc **Visualization:** We'll plot the embeddings using heatmaps."]}, {"cell_type": "code", "metadata": {}, "source": ["# Define an embedding layer (Embedding size = 8 for demonstration)\n", "embedding_layer = nn.Embedding(num_embeddings=tokenizer.vocab_size, embedding_dim=8)\n", "embedded_tokens = embedding_layer(torch.tensor(token_ids))\n", "\n", "# Convert embeddings to numpy for visualization\n", "embeddings_np = embedded_tokens.detach().numpy()\n", "\n", "# Plot heatmap of token embeddings\n", "plt.figure(figsize=(10, 6))\n", "sns.heatmap(embeddings_np, annot=True, cmap='coolwarm', fmt='.2f')\n", "plt.xlabel('Embedding Dimension')\n", "plt.ylabel('Tokens')\n", "plt.title('Token Embeddings Visualization')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udd39 Positional Encoding: Adding Order Information\n", "Since Transformers process words in parallel, positional encoding ensures word order is retained.\n", "### \ud83d\udccc **Visualization:** We'll plot the sine/cosine encoding matrix."]}, {"cell_type": "code", "metadata": {}, "source": ["def positional_encoding(seq_length, d_model):\n", "    pe = np.zeros((seq_length, d_model))\n", "    for pos in range(seq_length):\n", "        for i in range(0, d_model, 2):\n", "            pe[pos, i] = math.sin(pos / (10000 ** (i / d_model)))\n", "            pe[pos, i + 1] = math.cos(pos / (10000 ** (i / d_model)))\n", "    return torch.tensor(pe, dtype=torch.float32)\n", "\n", "# Apply positional encoding\n", "pos_encoding = positional_encoding(len(token_ids), 8).numpy()\n", "\n", "# Plot positional encoding matrix\n", "plt.figure(figsize=(10, 6))\n", "sns.heatmap(pos_encoding, cmap='coolwarm', xticklabels=False, yticklabels=False)\n", "plt.xlabel('Embedding Dimension')\n", "plt.ylabel('Position in Sentence')\n", "plt.title('Positional Encoding Visualization')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udd39 Self-Attention: Visualizing Attention Scores\n", "We'll compute and visualize the attention matrix."]}, {"cell_type": "code", "metadata": {}, "source": ["d_model = 8  # Embedding dimension\n", "\n", "# Define Q, K, V transformation matrices\n", "W_Q = nn.Linear(d_model, d_model)\n", "W_K = nn.Linear(d_model, d_model)\n", "W_V = nn.Linear(d_model, d_model)\n", "\n", "# Compute Q, K, V\n", "Q = W_Q(embedded_tokens)\n", "K = W_K(embedded_tokens)\n", "V = W_V(embedded_tokens)\n", "\n", "# Compute Attention Scores (Scaled Dot Product)\n", "scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_model)\n", "attention_weights = torch.nn.functional.softmax(scores, dim=-1).detach().numpy()\n", "\n", "# Plot attention heatmap\n", "plt.figure(figsize=(8, 6))\n", "sns.heatmap(attention_weights, cmap='coolwarm', xticklabels=tokens, yticklabels=tokens, annot=True)\n", "plt.xlabel('Key Tokens')\n", "plt.ylabel('Query Tokens')\n", "plt.title('Self-Attention Scores Visualization')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83c\udfaf Summary: Visualizing LLM Processing Steps\n", "- \u2705 **Tokenization:** Convert text into token IDs.\n", "- \u2705 **Embeddings:** Map tokens into dense numerical vectors (visualized with heatmaps).\n", "- \u2705 **Positional Encoding:** Adds order information using sine & cosine waves.\n", "- \u2705 **Self-Attention:** Computes relationships between words (visualized with attention heatmaps).\n", "\n", "\ud83d\ude80 **These visualizations make it easier to understand what happens inside a Transformer model!**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udd39 Visualizing Token Embeddings & Attention Weights\n", "To better understand the transformations, we'll use **matplotlib** to visualize:\n", "- **Token Embeddings** (Using PCA to reduce dimensions to 2D)\n", "- **Attention Weights** (Heatmap for attention scores between tokens)\n"]}, {"cell_type": "code", "metadata": {}, "source": ["from sklearn.decomposition import PCA\n", "import seaborn as sns\n", "\n", "# Reduce Embedding Dimensions for Visualization\n", "pca = PCA(n_components=2)\n", "embedded_tokens_np = embedded_tokens.detach().numpy()\n", "embedded_tokens_2d = pca.fit_transform(embedded_tokens_np)\n", "\n", "# Plot the embeddings\n", "plt.figure(figsize=(8, 5))\n", "for i, word in enumerate(tokens):\n", "    plt.scatter(embedded_tokens_2d[i, 0], embedded_tokens_2d[i, 1], label=word)\n", "    plt.text(embedded_tokens_2d[i, 0]+0.01, embedded_tokens_2d[i, 1]+0.01, word, fontsize=12)\n", "plt.title('\ud83d\udcca Token Embeddings in 2D Space')\n", "plt.xlabel('PCA Component 1')\n", "plt.ylabel('PCA Component 2')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udd39 Attention Weights Heatmap\n", "The attention scores determine how much each token **attends to every other token**."]}, {"cell_type": "code", "metadata": {}, "source": ["# Convert attention weights to numpy for visualization\n", "attention_weights_np = attention_weights.detach().numpy()\n", "\n", "# Plot heatmap of attention scores\n", "plt.figure(figsize=(8, 6))\n", "sns.heatmap(attention_weights_np[0], annot=True, cmap='coolwarm', xticklabels=tokens, yticklabels=tokens)\n", "plt.title('\ud83d\udd0d Attention Weight Heatmap')\n", "plt.xlabel('Key Tokens')\n", "plt.ylabel('Query Tokens')\n", "plt.show()"]}], "metadata": {}, "nbformat": 4, "nbformat_minor": 4}