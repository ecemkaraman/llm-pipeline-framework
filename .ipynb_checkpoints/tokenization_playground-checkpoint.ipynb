{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83d\ude80 Tokenization Playground: Explore Different Tokenizers in Python\n", "### \ud83d\udccc Understand how text is tokenized, converted into token IDs, and decoded back\n", "This notebook lets you experiment with **different tokenization methods**, compare them, and understand how LLMs process text inputs."]}, {"cell_type": "code", "metadata": {}, "source": ["# Install required libraries if not installed\n", "!pip install transformers tokenizers sentencepiece tiktoken"]}, {"cell_type": "code", "metadata": {}, "source": ["from transformers import AutoTokenizer\n", "import tiktoken\n", "import sentencepiece as spm\n", "import os\n", "import json\n", "from tokenizers import Tokenizer\n", "from tokenizers.models import BPE\n", "from tokenizers.trainers import BpeTrainer\n", "from tokenizers.pre_tokenizers import Whitespace\n", "import warnings\n", "warnings.filterwarnings('ignore')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udd39 Define Sample Input Text\n", "Let's define the text we will tokenize across different tokenizers."]}, {"cell_type": "code", "metadata": {}, "source": ["# Define sample text\n", "sample_text = \"Tokenization is an essential process in NLP and LLMs!\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udd39 OpenAI GPT-4 Tokenizer (Byte-Level BPE) \n", "We use `tiktoken`, which is OpenAI's tokenizer for GPT models."]}, {"cell_type": "code", "metadata": {}, "source": ["# Load GPT-4 tokenizer\n", "enc = tiktoken.encoding_for_model(\"gpt-4\")\n", "\n", "# Tokenize text\n", "tokens = enc.encode(sample_text)\n", "decoded_text = enc.decode(tokens)\n", "\n", "print(\"\ud83d\udd39 Token IDs:\", tokens)\n", "print(\"\ud83d\udd39 Decoded Text:\", decoded_text)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udd39 BERT Tokenizer (WordPiece)\n", "BERT models use **WordPiece Tokenization**, which breaks words into subwords based on frequency."]}, {"cell_type": "code", "metadata": {}, "source": ["# Load BERT tokenizer (WordPiece)\n", "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n", "\n", "# Tokenize text\n", "bert_tokens = bert_tokenizer(sample_text, return_tensors=\"pt\")\n", "\n", "print(\"\ud83d\udd39 Token IDs:\", bert_tokens[\"input_ids\"][0].tolist())\n", "print(\"\ud83d\udd39 Tokens:\", bert_tokenizer.convert_ids_to_tokens(bert_tokens[\"input_ids\"][0].tolist()))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udd39 SentencePiece Tokenizer (Used in T5, ALBERT)\n", "This tokenizer is used in **T5, XLNet, and ALBERT**, and supports both BPE and Unigram LM."]}, {"cell_type": "code", "metadata": {}, "source": ["# Train a SentencePiece model (Unigram)\n", "spm.SentencePieceTrainer.train(input=\"sample_text.txt\", model_prefix=\"m\", vocab_size=5000)\n", "sp = spm.SentencePieceProcessor(model_file='m.model')\n", "\n", "# Tokenize text\n", "tokens = sp.encode(sample_text, out_type=str)\n", "ids = sp.encode(sample_text)\n", "\n", "print(\"\ud83d\udd39 Tokens:\", tokens)\n", "print(\"\ud83d\udd39 Token IDs:\", ids)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udd39 Custom Byte Pair Encoding (BPE) Tokenization\n", "Train a **custom BPE tokenizer** using Hugging Face's `tokenizers` library."]}, {"cell_type": "code", "metadata": {}, "source": ["# Create a custom BPE tokenizer\n", "tokenizer = Tokenizer(BPE())\n", "tokenizer.pre_tokenizer = Whitespace()\n", "trainer = BpeTrainer(special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n", "\n", "# Train tokenizer on a sample file\n", "tokenizer.train([\"sample_text.txt\"], trainer)\n", "\n", "# Tokenize text\n", "output = tokenizer.encode(sample_text)\n", "print(\"\ud83d\udd39 Tokens:\", output.tokens)\n", "print(\"\ud83d\udd39 Token IDs:\", output.ids)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83c\udfaf Summary: Key Differences Between Tokenization Methods\n", "| **Tokenizer Type** | **Used in Models** | **How It Works?** |\n", "|----------------|------------------|----------------|\n", "| **Byte Pair Encoding (BPE)** | GPT-4, LLaMA-3, GPT-3 | Merges frequent subword pairs iteratively |\n", "| **WordPiece** | BERT, RoBERTa, DistilBERT | Uses probability to merge subwords |\n", "| **Unigram LM** | T5, XLNet, ALBERT | Drops unnecessary subwords probabilistically |\n", "| **SentencePiece** | T5, MarianMT | Works without whitespace dependency |\n", "| **Byte-Level BPE** | GPT-2, GPT-3, GPT-4 | BPE operating at byte level |\n", "\n", "\ud83d\ude80 **Now, try tokenizing different sentences to compare the outputs!**"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.8"}}, "nbformat": 4, "nbformat_minor": 4}